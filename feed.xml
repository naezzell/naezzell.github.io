<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://naezzell.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://naezzell.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-17T21:07:38+00:00</updated><id>https://naezzell.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Entry-wise vector and matrix derivatives</title><link href="https://naezzell.github.io/blog/2021/entrywise-matrix-derivative/" rel="alternate" type="text/html" title="Entry-wise vector and matrix derivatives"/><published>2021-08-30T00:00:00+00:00</published><updated>2021-08-30T00:00:00+00:00</updated><id>https://naezzell.github.io/blog/2021/entrywise-matrix-derivative</id><content type="html" xml:base="https://naezzell.github.io/blog/2021/entrywise-matrix-derivative/"><![CDATA[<p>During my time at USC, I took a machine learning course in Fall 2021. In the first homework assignment, we were given</p> <p>\begin{equation} f(\boldsymbol{x}) = \boldsymbol{x}^T A \boldsymbol{x} + \boldsymbol{b}^T \boldsymbol{x} \end{equation} for $\boldsymbol{x} \in \mathbb{R}^n$, $\boldsymbol{x}^T$ is the tranpose of this vector, and hence $f(\boldsymbol{x}) : \mathbb{R}^n \to \mathbb{R}$ is a scalar function with vector inputs.</p> <p>We were then asked to compute \begin{equation} \frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}, \qquad \frac{\partial f(\boldsymbol{x})}{\partial A}, \end{equation} which many people in the course found confusing notation since, at the end of the day,</p> <blockquote> <p>derivatives always act on scalar functions.</p> </blockquote> <p>The confusion is merely notational, as we shall see. In fact, this is a great example of the concepts of overloading notation and abuse of notation (hence the tags).</p> <hr/> <h2 id="vector-derivative-gradient-entry-wise">Vector derivative (gradient, entry-wise)</h2> <p>Since $f$ depends on the scalar variables $x_1, x_2, \ldots, x_n$, the most natural thing to do is to take all first partial derivatives:</p> <p>\begin{equation} \label{eq:guess-for-first-der} \frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_n}. \end{equation} Each term in Eq.\eqref{eq:guess-for-first-der} is an ordinary scalar derivative: we differentiate a scalar function with respect to a scalar variable.</p> <p>A convenient way to store these derivatives is in a vector, and that is exactly what “derivative with respect to a vector” means:</p> <p>\begin{equation} \label{eq:vec-div-def} \frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}} \equiv \left( \frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_n} \right)^T, \end{equation} where $\equiv$ is notation meaning ``defines” which is defining the notation for the vector derivative in this case. In fact, this is just the familiar [<a href="https://en.wikipedia.org/wiki/Gradient">gradient</a>] from multivariable calculus disguised by overloaded notation/an <em>abuse of notation</em>. Oh, and the tranpose is just to make this row vector a column vector—a minor detail for keeping track of dimensions properly.</p> <p>Thus your initial intuition that might have said “you can’t take a derivative with respect to a vector” is correct. The notation here simply instructs you to take all scalar derivatives and stack them into a vector, aka the gradient.</p> <hr/> <h2 id="matrix-derivative-entry-wise">Matrix derivative (entry-wise)</h2> <p>In exact analogy,</p> \[\frac{\partial f}{\partial A} = \left( \begin{array}{cccc} \frac{\partial f}{\partial A_{11}} &amp; \frac{\partial f}{\partial A_{12}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{1n}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f}{\partial A_{n1}} &amp; \frac{\partial f}{\partial A_{n2}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{nn}} \end{array} \right).\] <p>That is: take the scalar derivative with respect to each entry $A_{ij}$ and arrange the results into a matrix.</p> <hr/> <h2 id="a-concrete-2-times-2-example">A concrete $2 \times 2$ example</h2> <p>To drive the point home, let’s compute $\frac{\partial f(\boldsymbol{x})}{\partial x_1}$ for a $2 \times 2$ example. Suppose</p> \[\begin{aligned} f(\mathbf{x}) &amp;= (x_1, x_2) \left( \begin{array}{cc} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{array} \right) \left( \begin{array}{c} x_1 \\ x_2 \end{array} \right) \\ &amp;= A_{11} x_1^2 + A_{12} x_1 x_2 + A_{21} x_1 x_2 + A_{22} x_2^2. \end{aligned}\] <p>Then</p> \[\frac{\partial f}{\partial x_1} = 2 A_{11} x_1 + A_{12} x_2 + A_{21} x_2.\] <p>With this (and the analogous calculation for $\frac{\partial f}{\partial x_2}$), we have computed $\partial f / \partial \boldsymbol{x}$ for this $2 \times 2$ example. A similar exercise yields $\partial f / \partial A$. By writing out $f(\boldsymbol{x})$ for general $n$ or pattern matching by direct computation on the $2 \times 2, 3 \times 3, \ldots, n \times n$ answers, we can thus compute $\partial f / \partial \boldsymbol{x}$ and $\partial f / \partial A$ for general $n$, which we leave as an exercise given its origin.</p>]]></content><author><name></name></author><category term="math-notes"/><category term="abuse-of-notation"/><category term="operator-overloading"/><category term="calculus"/><category term="linear-algebra"/><summary type="html"><![CDATA[Uncovering ambiguous notation involving vector and matrix derivatives]]></summary></entry><entry><title type="html">Interaction picture</title><link href="https://naezzell.github.io/blog/2021/interaction-picture/" rel="alternate" type="text/html" title="Interaction picture"/><published>2021-01-28T00:00:00+00:00</published><updated>2021-01-28T00:00:00+00:00</updated><id>https://naezzell.github.io/blog/2021/interaction-picture</id><content type="html" xml:base="https://naezzell.github.io/blog/2021/interaction-picture/"><![CDATA[<p>Suppose you split a Hamiltonian into two parts,</p> \[H = H_S + H_{SB},\] <p>where $H_S$ acts on the system of interest and $H_{SB}$ characterizes the interaction between the system and the bath.</p> <p>The total system–bath density matrix satisfies the Liouville–von Neumann equation</p> \[\dot{\rho}(t) = -i [H, \rho(t)].\] <p>But suppose we are only interested in the dynamics induced from $H_{SB}$ specifically. It turns out we can effectively remove the $H_S$ contribution by entering the aptly named <strong>interaction picture</strong>, which is a special choice of a [<a href="https://physics.stackexchange.com/questions/222104/what-is-the-interaction-picture-or-rotating-frame-in-quantum-mechanics">rotating frame</a>].</p> <hr/> <h2 id="interaction-picture-state">Interaction picture state</h2> <p>Instead of studying $\rho(t)$, we define the interaction-picture state</p> \[\tilde{\rho}(t) = U_S^\dagger(t)\,\rho(t)\,U_S(t),\] <p>where</p> \[U_S(t) = e^{-i H_S t},\] <p>assuming $\dot{H}_S = 0$.</p> <p>This transformation rotates the density matrix into a time-dependent basis. Intuitively, we are <em>unrotating</em> the bare $H_S$ dynamics so that only the effect of $H_{SB}$ remains.</p> <hr/> <h2 id="interaction-picture-evolution">Interaction picture evolution</h2> <p>The transformed state satisfies</p> \[\dot{\tilde{\rho}}(t) = -i [\tilde{H}(t), \tilde{\rho}(t)],\] <p>where the interaction-picture Hamiltonian is</p> \[\tilde{H}(t) = U_S^\dagger(t)\, H_{SB}\, U_S(t).\] <p>So the dynamics are generated solely by the interaction Hamiltonian expressed in the rotating frame.</p> <hr/> <h2 id="proof">Proof</h2> <p>We differentiate</p> \[\tilde{\rho}(t) = U_S^\dagger(t)\,\rho(t)\,U_S(t)\] <p>using the product rule</p> \[\begin{aligned} \dot{\tilde{\rho}}(t) &amp;= \dot{U}_S^\dagger(t)\,\rho(t)\,U_S(t) + U_S^\dagger(t)\,\dot{\rho}(t)\,U_S(t) + U_S^\dagger(t)\,\rho(t)\,\dot{U}_S(t). \end{aligned}\] <p>Using</p> \[\dot{U}_S(t) = -i H_S U_S(t), \qquad \dot{U}_S^\dagger(t) = i H_S U_S^\dagger(t),\] <p>and substituting the Liouville equation $\dot{\rho}(t) = -i[H,\rho(t)]$, we obtain</p> \[\begin{aligned} \dot{\tilde{\rho}}(t) &amp;= (i H_S U_S^\dagger)\rho U_S + U_S^\dagger (-i[H,\rho]) U_S + U_S^\dagger \rho (-i U_S H_S). \end{aligned}\] <p>Factor out $U_S^\dagger$ and $U_S$</p> \[\dot{\tilde{\rho}}(t) = U_S^\dagger \left( i H_S \rho - i[H,\rho] - i \rho H_S \right) U_S.\] <p>Since $H = H_S + H_{SB}$, expand the commutator</p> \[[H,\rho] = [H_S,\rho] + [H_{SB},\rho].\] <p>The $H_S$ terms cancel, leaving</p> \[\dot{\tilde{\rho}}(t) = U_S^\dagger \left( -i H_{SB}\rho + i \rho H_{SB} \right) U_S.\] <p>Now insert the identity $I = U_S U_S^\dagger$ conveniently</p> \[\begin{aligned} \dot{\tilde{\rho}}(t) &amp;= -i U_S^\dagger H_{SB} (U_S U_S^\dagger) \rho U_S + i U_S^\dagger \rho (U_S U_S^\dagger) H_{SB} U_S. \end{aligned}\] <p>Recognizing</p> \[\tilde{\rho} = U_S^\dagger \rho U_S, \qquad \tilde{H} = U_S^\dagger H_{SB} U_S,\] <p>we obtain</p> \[\dot{\tilde{\rho}}(t) = -i \tilde{H} \tilde{\rho} + i \tilde{\rho} \tilde{H} = -i[\tilde{H}, \tilde{\rho}(t)].\] <hr/> <h2 id="interpretation">Interpretation</h2> <p>The interaction picture cleanly separates</p> <ul> <li>The <strong>free evolution</strong> generated by $H_S$</li> <li>The <strong>interaction dynamics</strong> generated by $H_{SB}$</li> </ul> <p>By moving to the rotating frame defined by $H_S$, we isolate the physical effect of the system–bath coupling.</p> <hr/> <h2 id="use-of-interaction-picture-to-analyze-dynamical-decoupling">Use of interaction picture to analyze dynamical decoupling</h2> <p>The interaction picture is routinely used in the quantum computation literature, but the area I am the most familiar with is is in dynamical decoupling (DD). In this case, we usually enter the interaction picture induced by the control Hamiltonian, $H_c(t)$, that is thus often called the <strong>toggling frame.</strong> A list of historical references for its use in NMR and early DD papers can be found in</p> <ul> <li>Appendix D of [<a href="https://arxiv.org/pdf/2207.03670">arXiv:2207.03670</a>]</li> </ul> <p>and a thorough description of its use in the “standard analysis pipeline of DD” can be found in</p> <ul> <li>Section III of [<a href="https://arxiv.org/pdf/0911.3202">arXiv:0911.3202</a>].</li> </ul> <p>A very intereting interaction picture (but specifically <em>not</em> just the toggling frame) analysis of DD for superconducting qubits is carried out in</p> <ul> <li>Appendix G and H of [<a href="https://arxiv.org/pdf/2108.04530">arXiv:2108.04530</a>].</li> </ul>]]></content><author><name></name></author><category term="quantum"/><category term="basic-quantum"/><category term="quantum-computation"/><summary type="html"><![CDATA[A brief description of the interaction picture in quantum mechanics and some references to papers that use it in the quantum computing literature]]></summary></entry></feed>