<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://naezzell.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://naezzell.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-17T22:06:43+00:00</updated><id>https://naezzell.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Entry-wise vector and matrix derivatives</title><link href="https://naezzell.github.io/blog/2021/entrywise-matrix-derivative/" rel="alternate" type="text/html" title="Entry-wise vector and matrix derivatives"/><published>2021-08-30T00:00:00+00:00</published><updated>2021-08-30T00:00:00+00:00</updated><id>https://naezzell.github.io/blog/2021/entrywise-matrix-derivative</id><content type="html" xml:base="https://naezzell.github.io/blog/2021/entrywise-matrix-derivative/"><![CDATA[<p>During my time at USC, I took a machine learning course in Fall 2021. In the first homework assignment, we were given</p> <p>\begin{equation} f(\boldsymbol{x}) = \boldsymbol{x}^T A \boldsymbol{x} + \boldsymbol{b}^T \boldsymbol{x} \end{equation} for $\boldsymbol{x} \in \mathbb{R}^n$, $\boldsymbol{x}^T$ is the tranpose of this vector, and hence $f(\boldsymbol{x}) : \mathbb{R}^n \to \mathbb{R}$ is a scalar function with vector inputs.</p> <p>We were then asked to compute \begin{equation} \frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}, \qquad \frac{\partial f(\boldsymbol{x})}{\partial A}, \end{equation} which many people in the course found confusing notation since, at the end of the day,</p> <blockquote> <p>derivatives always act on scalar functions.</p> </blockquote> <p>The confusion is merely notational, as we shall see. In fact, this is a great example of the concepts of overloading notation and abuse of notation (hence the tags).</p> <hr/> <h2 id="vector-derivative-gradient-entry-wise">Vector derivative (gradient, entry-wise)</h2> <p>Since $f$ depends on the scalar variables $x_1, x_2, \ldots, x_n$, the most natural thing to do is to take all first partial derivatives:</p> <p>\begin{equation} \label{eq:guess-for-first-der} \frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_n}. \end{equation} Each term in Eq.\eqref{eq:guess-for-first-der} is an ordinary scalar derivative: we differentiate a scalar function with respect to a scalar variable.</p> <p>A convenient way to store these derivatives is in a vector, and that is exactly what “derivative with respect to a vector” means:</p> <p>\begin{equation} \label{eq:vec-div-def} \frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}} \equiv \left( \frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_n} \right)^T, \end{equation} where $\equiv$ is notation meaning ``defines” which is defining the notation for the vector derivative in this case. In fact, this is just the familiar [<a href="https://en.wikipedia.org/wiki/Gradient">gradient</a>] from multivariable calculus disguised by overloaded notation/an <em>abuse of notation</em>. Oh, and the tranpose is just to make this row vector a column vector—a minor detail for keeping track of dimensions properly.</p> <p>Thus your initial intuition that might have said “you can’t take a derivative with respect to a vector” is correct. The notation here simply instructs you to take all scalar derivatives and stack them into a vector, aka the gradient.</p> <hr/> <h2 id="matrix-derivative-entry-wise">Matrix derivative (entry-wise)</h2> <p>In exact analogy,</p> \[\frac{\partial f}{\partial A} = \left( \begin{array}{cccc} \frac{\partial f}{\partial A_{11}} &amp; \frac{\partial f}{\partial A_{12}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{1n}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f}{\partial A_{n1}} &amp; \frac{\partial f}{\partial A_{n2}} &amp; \cdots &amp; \frac{\partial f}{\partial A_{nn}} \end{array} \right).\] <p>That is: take the scalar derivative with respect to each entry $A_{ij}$ and arrange the results into a matrix.</p> <hr/> <h2 id="a-concrete-2-times-2-example">A concrete $2 \times 2$ example</h2> <p>To drive the point home, let’s compute $\frac{\partial f(\boldsymbol{x})}{\partial x_1}$ for a $2 \times 2$ example. Suppose</p> \[\begin{aligned} f(\mathbf{x}) &amp;= (x_1, x_2) \left( \begin{array}{cc} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{array} \right) \left( \begin{array}{c} x_1 \\ x_2 \end{array} \right) \\ &amp;= A_{11} x_1^2 + A_{12} x_1 x_2 + A_{21} x_1 x_2 + A_{22} x_2^2. \end{aligned}\] <p>Then</p> \[\frac{\partial f}{\partial x_1} = 2 A_{11} x_1 + A_{12} x_2 + A_{21} x_2.\] <p>With this (and the analogous calculation for $\frac{\partial f}{\partial x_2}$), we have computed $\partial f / \partial \boldsymbol{x}$ for this $2 \times 2$ example. A similar exercise yields $\partial f / \partial A$. By writing out $f(\boldsymbol{x})$ for general $n$ or pattern matching by direct computation on the $2 \times 2, 3 \times 3, \ldots, n \times n$ answers, we can thus compute $\partial f / \partial \boldsymbol{x}$ and $\partial f / \partial A$ for general $n$, which we leave as an exercise given its origin.</p>]]></content><author><name></name></author><category term="notation"/><category term="math"/><category term="abuse-of-notation"/><category term="operator-overloading"/><category term="calculus"/><category term="linear-algebra"/><summary type="html"><![CDATA[Uncovering ambiguous notation involving vector and matrix derivatives]]></summary></entry><entry><title type="html">What is a matrix exponential? / Operator functions</title><link href="https://naezzell.github.io/blog/2021/operator-functions/" rel="alternate" type="text/html" title="What is a matrix exponential? / Operator functions"/><published>2021-05-10T00:00:00+00:00</published><updated>2021-05-10T00:00:00+00:00</updated><id>https://naezzell.github.io/blog/2021/operator-functions</id><content type="html" xml:base="https://naezzell.github.io/blog/2021/operator-functions/"><![CDATA[<p>Like most people, I found my introductory quantum mechanics course confusing, in part due to new mathematical and ideas and notations being thrown at me in ways that felt like an aside. As an example, consider the Schrödinger equation</p> \[\frac{d}{dt} \ket{\psi(t)} = -i H \ket{\psi(t)}.\] <p>In my course, I was taught that if $H$ is time-independent, then the solution is <em>just</em></p> \[\ket{\psi(t)} = e^{-i H t} \ket{\psi(0)},\] <p>which follows <em>trivially</em> from the following observation</p> \[\begin{aligned} \frac{d}{dt} \ket{\psi(t)} &amp;= -i H e^{-i H t} \ket{\psi(0)} \\ &amp;= -i H \ket{\psi(t)}. \end{aligned}\] <p>But wait a minute… How can we just exponentiate a matrix? Ahh, my professor shared, <em>just Taylor expand</em> for any matrix $A$,</p> \[e^{A} \equiv \mathds{1} + X + \frac{1}{2} X^2 + \ldots,\] <p>and you will be convinced all is well.</p> <p>For me, all was not well. At this point, I had only seen Taylor series (and for the matter functions) acting on, at most, a list of scalars.</p> <p>A number of questions arose immediately</p> <ul> <li>Why does the Taylor series even work for a matrix?</li> <li>Does it converge? How do I know?</li> <li>Okay, even if I accept this, what does it <em>really</em> mean?</li> </ul> <p>And once I got confused about differentiating matrix exponentials, my faith in differentiating a vector also crumbled.</p> <ul> <li>Why should $\frac{d}{dt} \ket{\psi(t)}$ be unique?</li> <li>Shouldn’t it depend on the basis?</li> <li>What does <em>it</em> really mean?</li> </ul> <p>In short: <strong>what is actually going on with this notation?</strong> An adequate explanation, as we shall see, follows from the notion of spectral function, which in turn requires some knowledge of linear algebra that we shall assume.</p> <hr/> <h2 id="spectral-functions">Spectral Functions</h2> <p>To understand a proper definition of a matrix operator in quantum mechanics, you first need the <strong>spectral decomposition</strong>.</p> <p>Given a <strong>normal</strong> matrix $A$, we can write</p> \[A = \sum_a \lambda_a \ket{a}\bra{a},\] <p>where $\lambda_a$ are the eigenvalues of $A$ and $\ket{a}$ are the corresponding eigenvectors. That is,</p> \[A \ket{a} = \lambda_a \ket{a}.\] <p>Once you’ve written $A$ this way, you can define a <strong>spectral function</strong></p> \[f(A) \equiv \sum_a f(\lambda_a)\ket{a}\bra{a}.\] <p>In other words, $f$ acts only on the eigenvalues of $A$. In the context of quantum mechanics in which eigenvalues encode things you can measure in the lab, it makes a lot of sense for such functions to be the most relevant.</p> <h3 id="non-examples">Non-examples</h3> <p>Not all matrix functions are spectral functions, though. For example, given a matrix</p> \[M = \begin{pmatrix} M_{00} &amp; M_{01} \\ M_{10} &amp; M_{11} \end{pmatrix},\] <p>we could define</p> \[g(M) \equiv \begin{pmatrix} M_{00} &amp; M_{01}^2 \\ \sqrt{M_{10}} &amp; \pi \end{pmatrix},\] <p>which does some different operation to each entry of the matrix. A more obvious abuse of notation that is also useful, especially in programming, is to lift a scalar function to a matrix entry-wise, i.e.,</p> \[f(M) \equiv \begin{pmatrix} f(M_{00}) &amp; f(M_{01}) \\ f(M_{10}) &amp; f(M_{11}) \end{pmatrix},\] <p>for $f$ your favorite scalar function. Both of these operations produce another matrix, but they have nothing obvious to do with the eigenvalues of $M$, so it is <strong>not</strong> a spectral function.</p> <hr/> <h2 id="polynomials-are-spectral-functions-in-disguise">Polynomials Are Spectral Functions in Disguise</h2> <p>Many functions that don’t obviously look spectral actually are, which muddies the water of this notation even more. Consider</p> \[p(A) = A A,\] <p>which does not appear spectral, but it is. The trick is that you can write $A$ in spectral form,</p> \[\begin{align*} AA &amp;= \left(\sum_a \lambda_a \ket{a}\bra{a}\right)\left(\sum_b \lambda_b \ket{b}\bra{b}\right), \end{align*}\] <p>and upon simplifying using the orthnormality of the basis, we find</p> \[p(A) = \sum_a \lambda_a^2 \ket{a}\bra{a},\] <p>where we now see that $p(A)$ has the effect of squaring the eigenvalues of $A$. By an extremely common abuse of notation/ operator overloading, we often write $A^2 = A A$, and hence, we can say that $p(A) = A^2$ is a spectral function.</p> <p>By the same logic, <strong>all matrix polynomials are spectral functions in disguise.</strong></p> <hr/> <h2 id="returning-to-the-matrix-exponential">Returning to the matrix exponential</h2> <p>One spectral function of utmost importance is the matrix exponential</p> \[e^{A} \equiv \sum_a e^{\lambda_a} \ket{a}\bra{a}.\] <p>Unfortunately, it is not given a special new symbol, which is an abuse of notation/ operator overloading. If we Taylor expand the ordinary exponential,</p> \[e^{\lambda_a} = 1 + \lambda_a + \frac{1}{2!}\lambda_a^2 + \cdots,\] <p>and recall that matrix polynomials are spectral functions, we recover the power series definition from our professor :),</p> \[e^{A} = \mathds{1} + A + \frac{1}{2!}A^2 + \cdots.\] <p>So the infinite series definition is not mysterious — it is simply the spectral definition written in disguise.</p> <hr/> <h2 id="spectral-derivatives-ambiguous-notation-again">Spectral Derivatives: Ambiguous notation again</h2> <p>Finally, we point out that the power series could be written in terms of spectral derivatives. That is, if we define</p> \[\frac{d}{dt} A \equiv \sum_a \frac{d\lambda_a}{dt} \ket{a}\bra{a}.\] <p>then, we can find the power series expansion of $e^A$ by formal Taylor series expansion, and everything is self-consistent.</p> <p>However, we need to be careful. In the beginning, we wrote down the Schrödinger equation which uses</p> \[\frac{d}{dt} \ket{\psi(t)} = \left(\frac{d}{dt} \psi_0(t), \frac{d}{dt} \psi_1(t), \ldots, \frac{d}{dt} \psi_{d-1}(t)\right)^T.\] <p>which is actually an entry-wise derivative, as we discuss in a related <a href="/blog/2021/entrywise-matrix-derivative/" target="_blank">blog post</a>. This naively clashes with our spectral definition of the derivative which makes things awkward. Yet, there is a saving grace here, and things can be used self-consistently without care. This is both the power and the great confusion hiding in overloaded notation/ abuses of notation.</p> <p>The trick in this case is that a vector is really just a diagonal matrix in disguise, and a diagonal matrix is just a matrix of its own eigenvalues. In that sense, a spectral derivative acting on a vector <em>is</em> just the entry-wise derivative. Ahh, how convenient and confusing :).</p> <hr/> <h3 id="the-moral">The Moral</h3> <p>Matrix exponentials, derivatives, and other operator functions are not mysterious manipulations of symbols. They are functions acting on eigenvalues, expressed back in operator form. Once viewed through the spectral lens, the notation becomes far less magical – we return to functions acting on scalars. And in fact, in the context of quantum mechanics, it makes a lot of sense that those scalars are the eigenvalues which are experimentally measureable quantities.</p>]]></content><author><name></name></author><category term="notation"/><category term="quantum"/><category term="math"/><category term="basic-quantum"/><category term="abuse-of-notation"/><category term="operator-overloading"/><summary type="html"><![CDATA[A brief introduction to operator functions as used in quantum theory or]]></summary></entry><entry><title type="html">Interaction picture</title><link href="https://naezzell.github.io/blog/2021/interaction-picture/" rel="alternate" type="text/html" title="Interaction picture"/><published>2021-01-28T00:00:00+00:00</published><updated>2021-01-28T00:00:00+00:00</updated><id>https://naezzell.github.io/blog/2021/interaction-picture</id><content type="html" xml:base="https://naezzell.github.io/blog/2021/interaction-picture/"><![CDATA[<p>Suppose you split a Hamiltonian into two parts,</p> \[H = H_S + H_{SB},\] <p>where $H_S$ acts on the system of interest and $H_{SB}$ characterizes the interaction between the system and the bath.</p> <p>The total system–bath density matrix satisfies the Liouville–von Neumann equation</p> \[\dot{\rho}(t) = -i [H, \rho(t)].\] <p>But suppose we are only interested in the dynamics induced from $H_{SB}$ specifically. It turns out we can effectively remove the $H_S$ contribution by entering the aptly named <strong>interaction picture</strong>, which is a special choice of a [<a href="https://physics.stackexchange.com/questions/222104/what-is-the-interaction-picture-or-rotating-frame-in-quantum-mechanics">rotating frame</a>].</p> <hr/> <h2 id="interaction-picture-state">Interaction picture state</h2> <p>Instead of studying $\rho(t)$, we define the interaction-picture state</p> \[\tilde{\rho}(t) = U_S^\dagger(t)\,\rho(t)\,U_S(t),\] <p>where</p> \[U_S(t) = e^{-i H_S t},\] <p>assuming $\dot{H}_S = 0$.</p> <p>This transformation rotates the density matrix into a time-dependent basis. Intuitively, we are <em>unrotating</em> the bare $H_S$ dynamics so that only the effect of $H_{SB}$ remains.</p> <hr/> <h2 id="interaction-picture-evolution">Interaction picture evolution</h2> <p>The transformed state satisfies</p> \[\dot{\tilde{\rho}}(t) = -i [\tilde{H}(t), \tilde{\rho}(t)],\] <p>where the interaction-picture Hamiltonian is</p> \[\tilde{H}(t) = U_S^\dagger(t)\, H_{SB}\, U_S(t).\] <p>So the dynamics are generated solely by the interaction Hamiltonian expressed in the rotating frame.</p> <hr/> <h2 id="proof">Proof</h2> <p>We differentiate</p> \[\tilde{\rho}(t) = U_S^\dagger(t)\,\rho(t)\,U_S(t)\] <p>using the product rule</p> \[\begin{aligned} \dot{\tilde{\rho}}(t) &amp;= \dot{U}_S^\dagger(t)\,\rho(t)\,U_S(t) + U_S^\dagger(t)\,\dot{\rho}(t)\,U_S(t) + U_S^\dagger(t)\,\rho(t)\,\dot{U}_S(t). \end{aligned}\] <p>Using</p> \[\dot{U}_S(t) = -i H_S U_S(t), \qquad \dot{U}_S^\dagger(t) = i H_S U_S^\dagger(t),\] <p>and substituting the Liouville equation $\dot{\rho}(t) = -i[H,\rho(t)]$, we obtain</p> \[\begin{aligned} \dot{\tilde{\rho}}(t) &amp;= (i H_S U_S^\dagger)\rho U_S + U_S^\dagger (-i[H,\rho]) U_S + U_S^\dagger \rho (-i U_S H_S). \end{aligned}\] <p>Factor out $U_S^\dagger$ and $U_S$</p> \[\dot{\tilde{\rho}}(t) = U_S^\dagger \left( i H_S \rho - i[H,\rho] - i \rho H_S \right) U_S.\] <p>Since $H = H_S + H_{SB}$, expand the commutator</p> \[[H,\rho] = [H_S,\rho] + [H_{SB},\rho].\] <p>The $H_S$ terms cancel, leaving</p> \[\dot{\tilde{\rho}}(t) = U_S^\dagger \left( -i H_{SB}\rho + i \rho H_{SB} \right) U_S.\] <p>Now insert the identity $I = U_S U_S^\dagger$ conveniently</p> \[\begin{aligned} \dot{\tilde{\rho}}(t) &amp;= -i U_S^\dagger H_{SB} (U_S U_S^\dagger) \rho U_S + i U_S^\dagger \rho (U_S U_S^\dagger) H_{SB} U_S. \end{aligned}\] <p>Recognizing</p> \[\tilde{\rho} = U_S^\dagger \rho U_S, \qquad \tilde{H} = U_S^\dagger H_{SB} U_S,\] <p>we obtain</p> \[\dot{\tilde{\rho}}(t) = -i \tilde{H} \tilde{\rho} + i \tilde{\rho} \tilde{H} = -i[\tilde{H}, \tilde{\rho}(t)].\] <hr/> <h2 id="interpretation">Interpretation</h2> <p>The interaction picture cleanly separates</p> <ul> <li>The <strong>free evolution</strong> generated by $H_S$</li> <li>The <strong>interaction dynamics</strong> generated by $H_{SB}$</li> </ul> <p>By moving to the rotating frame defined by $H_S$, we isolate the physical effect of the system–bath coupling.</p> <hr/> <h2 id="use-of-interaction-picture-to-analyze-dynamical-decoupling">Use of interaction picture to analyze dynamical decoupling</h2> <p>The interaction picture is routinely used in the quantum computation literature, but the area I am the most familiar with is is in dynamical decoupling (DD). In this case, we usually enter the interaction picture induced by the control Hamiltonian, $H_c(t)$, that is thus often called the <strong>toggling frame.</strong> A list of historical references for its use in NMR and early DD papers can be found in</p> <ul> <li>Appendix D of [<a href="https://arxiv.org/pdf/2207.03670">arXiv:2207.03670</a>]</li> </ul> <p>and a thorough description of its use in the “standard analysis pipeline of DD” can be found in</p> <ul> <li>Section III of [<a href="https://arxiv.org/pdf/0911.3202">arXiv:0911.3202</a>].</li> </ul> <p>A very intereting interaction picture (but specifically <em>not</em> just the toggling frame) analysis of DD for superconducting qubits is carried out in</p> <ul> <li>Appendix G and H of [<a href="https://arxiv.org/pdf/2108.04530">arXiv:2108.04530</a>].</li> </ul>]]></content><author><name></name></author><category term="quantum"/><category term="math"/><category term="basic-quantum"/><category term="quantum-computation"/><summary type="html"><![CDATA[A brief description of the interaction picture in quantum mechanics and some references to papers that use it in the quantum computing literature]]></summary></entry></feed>